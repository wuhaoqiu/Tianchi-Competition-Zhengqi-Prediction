{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T02:32:04.839199Z",
     "start_time": "2019-04-28T02:32:04.834213Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T00:26:55.897136Z",
     "start_time": "2019-04-27T00:26:55.814362Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zhengqi_train=pd.read_csv('zhengqi_train.txt',sep='\\t')\n",
    "zhengqi_test=pd.read_csv('zhengqi_test.txt',sep='\\t')\n",
    "print(zhengqi_train.head())\n",
    "# zhengqi test donot have output\n",
    "print(zhengqi_test.head())\n",
    "print(zhengqi_train.shape)\n",
    "print(zhengqi_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T04:11:07.779450Z",
     "start_time": "2019-04-23T04:11:07.228303Z"
    }
   },
   "outputs": [],
   "source": [
    "# target is our purpose, so firstly plot it to see whether there exis outliers\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(zhengqi_train.shape[0]),np.sort(zhengqi_train['target']))\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('value_of_target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T04:11:08.780137Z",
     "start_time": "2019-04-23T04:11:08.307549Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.distplot(zhengqi_train['target'],bins=50,kde=True)\n",
    "plt.xlabel('value_of_target')\n",
    "# a little bit skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T04:11:09.105509Z",
     "start_time": "2019-04-23T04:11:09.081573Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate a datafrmae containing the index of column and the data type of each column\n",
    "feature_types=zhengqi_train.dtypes.reset_index()\n",
    "# rename column name of feature_types whose type is a dataframe\n",
    "feature_types.columns=['name_of_column','data_type']\n",
    "print(feature_types)\n",
    "feature_types.groupby('data_type').aggregate('count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T04:11:09.325705Z",
     "start_time": "2019-04-23T04:11:09.314703Z"
    }
   },
   "outputs": [],
   "source": [
    "# check missing values\n",
    "def check_missing_values_by_column(data):\n",
    "    assert isinstance(data,pd.DataFrame)\n",
    "    missing_values=data.isnull().sum(axis=0).reset_index()\n",
    "    missing_values=missing_values[missing_values.iloc[:,1]>0]\n",
    "    print(missing_values)\n",
    "\n",
    "check_missing_values_by_column(zhengqi_train)\n",
    "# empty,no missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T04:11:09.591742Z",
     "start_time": "2019-04-23T04:11:09.549896Z"
    }
   },
   "outputs": [],
   "source": [
    "# check duplicate rows\n",
    "zhengqi_train[zhengqi_train.drop(['target'],axis=1).duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T04:16:59.785341Z",
     "start_time": "2019-04-29T04:16:59.770384Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot to compare distribution of train set and test set\n",
    "def plot_each_column(zhengqi_train,zhengqi_test):\n",
    "    column_names=list(zhengqi_train)\n",
    "    length=len(column_names)\n",
    "    print(length)\n",
    "    for i in range(length):\n",
    "        if column_names[i]=='target':\n",
    "            pass\n",
    "        else:\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.subplot(1,3,1)\n",
    "            plt.scatter(range(zhengqi_train.shape[0]),np.sort(zhengqi_train[column_names[i]]))\n",
    "            plt.scatter(range(zhengqi_test.shape[0]),np.sort(zhengqi_test[column_names[i]]))\n",
    "            plt.xlabel('index')\n",
    "            plt.ylabel(column_names[i])\n",
    "            plt.legend(['train_set','test_set'])\n",
    "\n",
    "            plt.subplot(1,3,2)\n",
    "            sns.distplot(zhengqi_train[column_names[i]],bins=50,kde=True)\n",
    "            sns.distplot(zhengqi_test[column_names[i]],bins=50,kde=True)\n",
    "            plt.xlabel(column_names[i])\n",
    "            plt.legend(['train_set','test_set'])\n",
    "\n",
    "            plt.subplot(1,3,3)\n",
    "            plt.scatter(x=column_names[i], y='target', data=zhengqi_train)\n",
    "            plt.xlabel(column_names[i])\n",
    "            plt.ylabel('target')\n",
    "\n",
    "            plt.show()\n",
    "        \n",
    "# plot_each_column(zhengqi_train,zhengqi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T01:57:05.544376Z",
     "start_time": "2019-04-29T01:57:04.653761Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "第一步，把异常值去掉\n",
    "\"\"\"\n",
    "# drop outliers that greater than or smaller test set\n",
    "def drop_points_v1(train_set,test_set):\n",
    "    column_names=list(train_set)\n",
    "    for name in column_names[:-1]:\n",
    "        max_value_test=test_set[name].max()\n",
    "        min_value_test=test_set[name].min()\n",
    "#         找train dataset中每个column里最大最小的三个数\n",
    "        temp_min,temp_max=train_set.nsmallest(1,name),train_set.nlargest(3,name)\n",
    "        min_value_train=temp_min[name].max()\n",
    "        max_value_train=temp_max[name].min()\n",
    "#         print(max_value_train)\n",
    "        if max_value_train>max_value_test:\n",
    "            train_set=train_set[train_set[name]<max_value_train]\n",
    "        if min_value_train<min_value_test:\n",
    "            train_set=train_set[train_set[name]>min_value_train]\n",
    "    return train_set\n",
    "\n",
    "def drop_points_v2(train_set,test_set):\n",
    "    train_set=train_set.copy()\n",
    "    column_names=list(train_set)\n",
    "    for name in column_names[:-1]:\n",
    "        if name=='V9':\n",
    "            temp_min=train_set.nsmallest(2,name)\n",
    "            min_value_train=temp_min[name].max()\n",
    "            train_set=train_set[train_set[name]>min_value_train]\n",
    "        if name=='V10':\n",
    "            train_set=train_set[train_set[name]<3.6]\n",
    "        if name=='V15':\n",
    "            temp_min,temp_max=train_set.nsmallest(1,name),train_set.nlargest(3,name)\n",
    "            min_value_train=temp_min[name].max()\n",
    "            max_value_train=temp_max[name].min()\n",
    "            train_set=train_set[train_set[name]<max_value_train]\n",
    "            train_set=train_set[train_set[name]>min_value_train]\n",
    "        if name=='V17':\n",
    "            train_set=train_set[train_set[name]<1.7]\n",
    "        if name=='V23':\n",
    "            temp_max=train_set.nlargest(2,name)\n",
    "            max_value_train=temp_max[name].min()\n",
    "            train_set=train_set[train_set[name]<max_value_train]\n",
    "        if name=='V24':\n",
    "            temp_max=train_set.nlargest(6,name)\n",
    "            max_value_train=temp_max[name].min()\n",
    "            train_set=train_set[train_set[name]<max_value_train]\n",
    "        if name=='V29':\n",
    "            temp_max=train_set.nlargest(3,name)\n",
    "            max_value_train=temp_max[name].min()\n",
    "            train_set=train_set[train_set[name]<max_value_train]\n",
    "        if name=='V36':\n",
    "            temp_max=train_set.nlargest(1,name)\n",
    "            max_value_train=temp_max[name].min()\n",
    "            train_set=train_set[train_set[name]<max_value_train]\n",
    "    return train_set\n",
    "\n",
    "def process_outlier_in_testset(test_set):\n",
    "    test_set=test_set.copy()\n",
    "    column_names=list(test_set)\n",
    "    for name in column_names:\n",
    "        if name=='V21':\n",
    "            temp_min=test_set.nsmallest(2,name)\n",
    "            min_value=temp_min[name].max()\n",
    "            print(min_value)\n",
    "            index_mask=(test_set[name]<=min_value)\n",
    "            temp_min[name]=temp_min[name]*0.6\n",
    "            test_set[index_mask]=temp_min\n",
    "        if name=='V35':\n",
    "            temp_min=test_set.nsmallest(1,name)\n",
    "            min_value=temp_min[name].max()\n",
    "            index_mask=(test_set[name]<=min_value)\n",
    "            temp_min[name]=-5.9\n",
    "            test_set[index_mask]=temp_min\n",
    "    return test_set\n",
    "\n",
    "zhengqi_test_after_change_points=process_outlier_in_testset(zhengqi_test)\n",
    "zhengqi_train_after_drop_points=drop_points_v2(zhengqi_train,zhengqi_test_after_change_points)\n",
    "\n",
    "print(zhengqi_train_after_drop_points.shape)\n",
    "print(zhengqi_test_after_change_points.shape)\n",
    "# plot_each_column(zhengqi_train_after_drop_points,zhengqi_test_after_change_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T05:04:45.346448Z",
     "start_time": "2019-04-28T05:04:13.619361Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_each_column(zhengqi_train_after_drop_points,zhengqi_test_after_change_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T01:57:17.021695Z",
     "start_time": "2019-04-29T01:57:16.695538Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "转换装箱一些变量\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "enc=LabelEncoder()\n",
    "\n",
    "# def transform_categorical_columns(list_of_columns,train_sest,test_set):\n",
    "#     for name in list_of_columns:\n",
    "\n",
    "num_cut=[-7+0.5*i for i in range(20)]\n",
    "group_name=[str(i) for i in range(19)]\n",
    "zhengqi_train_after_drop_points[\"V9cut\"]=pd.cut(zhengqi_train_after_drop_points[\"V9\"],num_cut,labels=group_name)\n",
    "zhengqi_test_after_change_points[\"V9cut\"]=pd.cut(zhengqi_test_after_change_points[\"V9\"],num_cut,labels=group_name)\n",
    "zhengqi_train_after_drop_points['V9cutlabel']=enc.fit_transform(zhengqi_train_after_drop_points['V9cut'])\n",
    "zhengqi_test_after_change_points['V9cutlabel']=enc.fit_transform(zhengqi_test_after_change_points['V9cut'])\n",
    "zhengqi_train_after_drop_points = zhengqi_train_after_drop_points.drop([\"V9\", \"V9cut\"], axis=1)\n",
    "zhengqi_test_after_change_points=zhengqi_test_after_change_points.drop([\"V9\", \"V9cut\"], axis=1)\n",
    "\n",
    "num_cut_v23=[-6+0.5*i for i in range(17)]\n",
    "group_name_v23=[str(i) for i in range(16)]\n",
    "zhengqi_train_after_drop_points[\"V23cut\"]=pd.cut(zhengqi_train_after_drop_points[\"V23\"],num_cut_v23,labels=group_name_v23)\n",
    "zhengqi_test_after_change_points[\"V23cut\"]=pd.cut(zhengqi_test_after_change_points[\"V23\"],num_cut_v23,labels=group_name_v23)\n",
    "zhengqi_train_after_drop_points['V23cutlabel']=enc.fit_transform(zhengqi_train_after_drop_points['V23cut'])\n",
    "zhengqi_test_after_change_points['V23cutlabel']=enc.fit_transform(zhengqi_test_after_change_points['V23cut'])\n",
    "zhengqi_train_after_drop_points = zhengqi_train_after_drop_points.drop([\"V23\", \"V23cut\"], axis=1)\n",
    "zhengqi_test_after_change_points=zhengqi_test_after_change_points.drop([\"V23\", \"V23cut\"], axis=1)\n",
    "\n",
    "assert zhengqi_test_after_change_points.shape[1]==(zhengqi_train_after_drop_points.shape[1]-1)\n",
    "print(zhengqi_test_after_change_points.columns)\n",
    "print(zhengqi_train_after_drop_points.columns)\n",
    "print(zhengqi_train_after_drop_points['V9cutlabel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T01:57:28.610697Z",
     "start_time": "2019-04-29T01:57:28.491974Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "增加与categorical相关的组合特征\n",
    "\"\"\"\n",
    "list_of_continous_columns=['V0','V1','V2','V3','V4','V6','V7','V8','V10','V12','V15','V16','V19','V20','V29','V31','V36','V37']\n",
    "\n",
    "def combine_with_v9(list_of_columns,data_set,test_set):\n",
    "    data_set=data_set.copy()\n",
    "    test_set=test_set.copy()\n",
    "    for name in list_of_columns:\n",
    "        new_name='V9'+name\n",
    "        data_set[new_name]=data_set['V9cutlabel']*data_set[name]\n",
    "        test_set[new_name]=test_set['V9cutlabel']*test_set[name]\n",
    "    return data_set,test_set\n",
    "\n",
    "\n",
    "def combine_with_v23(list_of_columns,data_set,test_set):\n",
    "    data_set=data_set.copy()\n",
    "    test_set=test_set.copy()\n",
    "    for name in list_of_columns:\n",
    "        new_name='V23'+name\n",
    "        data_set[new_name]=data_set['V23cutlabel']*data_set[name]\n",
    "        test_set[new_name]=test_set['V23cutlabel']*test_set[name]\n",
    "    return data_set,test_set\n",
    "\n",
    "def combine_with_v24(list_of_columns,data_set,test_set):\n",
    "    data_set=data_set.copy()\n",
    "    test_set=test_set.copy()\n",
    "    for name in list_of_columns:\n",
    "        new_name='V24'+name\n",
    "        data_set[new_name]=data_set['V24cutlabel']*data_set[name]\n",
    "        test_set[new_name]=test_set['V24cutlabel']*test_set[name]\n",
    "    return data_set,test_set\n",
    "\n",
    "def combine_with_v28(list_of_columns,data_set,test_set):\n",
    "    data_set=data_set.copy()\n",
    "    test_set=test_set.copy()\n",
    "    for name in list_of_columns:\n",
    "        new_name='V28'+name\n",
    "        data_set[new_name]=data_set['V28cutlabel']*data_set[name]\n",
    "        test_set[new_name]=test_set['V28cutlabel']*test_set[name]\n",
    "    return data_set,test_set\n",
    "\n",
    "def combine_with_v35(list_of_columns,data_set,test_set):\n",
    "    data_set=data_set.copy()\n",
    "    test_set=test_set.copy()\n",
    "    for name in list_of_columns:\n",
    "        new_name='V35'+name\n",
    "        data_set[new_name]=data_set['V35cutlabel']*data_set[name]\n",
    "        test_set[new_name]=test_set['V35cutlabel']*test_set[name]\n",
    "    return data_set,test_set\n",
    "\n",
    "\n",
    "def combine_with_v9_v2(list_of_columns,data_set,test_set):\n",
    "    data_set=data_set.copy()\n",
    "    test_set=test_set.copy()\n",
    "    accumulated_value_train=0\n",
    "    accumulated_value_test=0\n",
    "    for name in list_of_columns:\n",
    "        new_name='V9cutlabel'+name\n",
    "        accumulated_value_train=accumulated_value_train+data_set[name]\n",
    "        accumulated_value_test=accumulated_value_test+test_set[name]\n",
    "    data_set[new_name]=data_set['V9cutlabel']*accumulated_value_train\n",
    "    test_set[new_name]=test_set['V9cutlabel']*accumulated_value_test\n",
    "    return data_set,test_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T01:57:38.782468Z",
     "start_time": "2019-04-29T01:57:38.754512Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "增加连续变量平方变化\n",
    "\"\"\"\n",
    "\n",
    "def add_squared_feature(column_names,train_set,test_set):\n",
    "    train_set=train_set.copy()\n",
    "    test_set=test_set.copy()\n",
    "    for name in column_names:\n",
    "        new_name='Squared'+name\n",
    "        train_set[new_name]=train_set[name]*train_set[name]\n",
    "        test_set[new_name]=test_set[name]*test_set[name]\n",
    "    return train_set,test_set\n",
    "\n",
    "zhengqi_train_after_drop_points,zhengqi_test_after_change_points=add_squared_feature(list_of_continous_columns,zhengqi_train_after_drop_points,zhengqi_test_after_change_points)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T01:57:49.074909Z",
     "start_time": "2019-04-29T01:57:49.068926Z"
    }
   },
   "outputs": [],
   "source": [
    "print(zhengqi_test_after_change_points.columns)\n",
    "print(zhengqi_train_after_drop_points.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T05:09:53.545536Z",
     "start_time": "2019-04-28T05:09:53.515615Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "可选，drop columns去生成多个训练集，可选，经实践证明，不好用\n",
    "\"\"\"\n",
    "\n",
    "# drop features whose distributions are higly different\n",
    "\n",
    "# 由于V5和V11高度线性相关，所以drop掉了分布非常不匹配的V5\n",
    "list_of_columns1=['V5']\n",
    "list_of_columns2=['V5','V22']\n",
    "list_of_columns3=['V5','V11','V17','V22','V27']\n",
    "list_of_columns4=['V5','V14','V21','V27','V32','V33']\n",
    "list_of_columns5=['V5','V11','V14','V17','V22','V27']\n",
    "# list_of_columnsx=['V5','V9','V9cut','V14','V22']\n",
    "# list_of_columnsx=['V5','V14','V22']\n",
    "# list_of_columnsx=['V4','V5','V11','V13','V19','V21','V22','V26','V28','V9','V17','V9cut','V17cut','V35','V35cut']\n",
    "list_of_columnsx=['V4','V5','V11','V13','V19','V21','V22','V26','V28','V9cutlabel','V17cutlabel','V9cut','V17cut','V35cutlabel','V35cut','V17']\n",
    "zhengqi_train_after_drop_points_try,zhengqi_test_try=drop_columns(list_of_columnsx,zhengqi_train_after_drop_points,zhengqi_test_after_change_points)\n",
    "\n",
    "def drop_columns(list_of_columns,train_data_frame,test_data_frame):\n",
    "    zhengqi_train_after_drop=train_data_frame.drop(list_of_columns,axis=1)\n",
    "    zhengqi_test_after_drop=test_data_frame.drop(list_of_columns,axis=1)\n",
    "    return zhengqi_train_after_drop,zhengqi_test_after_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T00:17:11.589759Z",
     "start_time": "2019-04-27T00:17:11.548883Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_highly_correlated_pairs(data_frame,first_n):\n",
    "    assert isinstance(data_frame,pd.DataFrame)\n",
    "    \n",
    "    print(\"找出最相关参数\")\n",
    "    corr = data_frame.corr()\n",
    "    corr.sort_values([\"target\"], ascending = False, inplace = True)\n",
    "    print(corr.target)\n",
    "    \n",
    "    print('other related feature pairs')\n",
    "    data_frame_cor=data_frame.corr().abs().unstack().sort_values(kind=\"quicksort\",ascending=False)\n",
    "    data_frame_cor_filtered=data_frame_cor[data_frame_cor>0]\n",
    "    data_frame_cor_filtered=data_frame_cor_filtered[data_frame_cor_filtered<1]\n",
    "#     print(data_frame_cor_filtered)\n",
    "    print(data_frame_cor_filtered[:first_n:2])\n",
    "    \n",
    "print_highly_correlated_pairs(zhengqi_train_after_drop_points,1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T04:38:53.331698Z",
     "start_time": "2019-04-28T04:38:53.281845Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "可选，归一化或不归一化两种测试集，并把target单独拿出来\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "PCA降维处理\n",
    "\"\"\"\n",
    "# split features and target\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train_,test_=train_test_split(zhengqi_train_after_drop_points,test_size=0.2,random_state=2)\n",
    "# test_x,test_y=split_feature_label(test_,'target')\n",
    "# test_x_scaled=pd.DataFrame(scaler.fit_transform(test_x))\n",
    "\n",
    "def scale_data(train_x,test_x):\n",
    "    from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "    scaler=MinMaxScaler()\n",
    "    scaler.fit(train_x)\n",
    "    train_x_scaled=pd.DataFrame(scaler.transform(train_x))\n",
    "    test_x_scaled=pd.DataFrame(scaler.transform(test_x))\n",
    "    return train_x_scaled,test_x_scaled\n",
    "    \n",
    "def split_feature_label(dataset,column_name):\n",
    "    feature_columns=dataset.drop(column_name,axis=1)\n",
    "    return feature_columns,dataset.loc[:,column_name]\n",
    "\n",
    "def pca_process(train_x,test_x):\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=0.95)\n",
    "    pca.fit(train_x)\n",
    "    train_x_pca = pca.transform(train_x)\n",
    "    test_x_pca = pca.transform(test_x)\n",
    "    return train_x_pca,test_x_pca\n",
    "\n",
    "\n",
    "assert train_y_without5.shape[0]==zhengqi_train_after_drop_points_without5.shape[0]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "scaler_without5=MinMaxScaler()\n",
    "scaler_without5.fit(train_x_without5)\n",
    "train_x_without5_scaled=pd.DataFrame(scaler_without5.transform(train_x_without5))\n",
    "zhengqi_test_without5_scaled=pd.DataFrame(scaler_without5.transform(zhengqi_test_without5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(train_x,test_x):\n",
    "    from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "    scaler=MinMaxScaler()\n",
    "    scaler.fit(train_x)\n",
    "    train_x_scaled=pd.DataFrame(scaler.transform(train_x))\n",
    "    test_x_scaled=pd.DataFrame(scaler.transform(test_x))\n",
    "    return train_x_scaled,test_x_scaled\n",
    "    \n",
    "def split_feature_label(dataset,column_name):\n",
    "    feature_columns=dataset.drop(column_name,axis=1)\n",
    "    return feature_columns,dataset.loc[:,column_name]\n",
    "\n",
    "def drop_columns(list_of_columns,train_data_frame,test_data_frame):\n",
    "    zhengqi_train_after_drop=train_data_frame.drop(list_of_columns,axis=1)\n",
    "    zhengqi_test_after_drop=test_data_frame.drop(list_of_columns,axis=1)\n",
    "    return zhengqi_train_after_drop,zhengqi_test_after_drop\n",
    "\n",
    "list_of_columnsx=['V5','V11','V27','V17','V22']\n",
    "zhengqi_train_after_drop_points_try,zhengqi_test_try=drop_columns(list_of_columnsx,zhengqi_train_after_drop_points,zhengqi_test_after_change_points)\n",
    "train_x_try,train_y_try=split_feature_label(zhengqi_train_after_drop_points_try,'target')\n",
    "train_x_try_scaled,zhengqi_test_try_scaled=scale_data(train_x_try,zhengqi_test_try)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T03:13:28.358697Z",
     "start_time": "2019-04-29T03:13:28.349706Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "第四步，创建index mask为最后的预测整合做准备\n",
    "\"\"\"\n",
    "# 1925*1,record indexes\n",
    "record=np.zeros((zhengqi_test_after_change_points.shape[0],1))\n",
    "\n",
    "# 用没有归一化，但是又处理了异常值的train和test set\n",
    "for name in ['V27']:\n",
    "    min_value_train=zhengqi_train_after_drop_points[name].min()\n",
    "    max_value_train=zhengqi_train_after_drop_points[name].max()\n",
    "    range_of_column=max_value_train-min_value_train\n",
    "#     如果test中的一行在training set中对应的column的范围内，那么该index所在行就乘上对应的编号\n",
    "    if name=='V11':\n",
    "        record11=((zhengqi_test_after_change_points[name]<=max_value_train*0.7)&(zhengqi_test_after_change_points[name]>=min_value_train*0.7))*11\n",
    "    if name=='V17':\n",
    "        record17=((zhengqi_test_after_change_points[name]<=max_value_train*0.6)&(zhengqi_test_after_change_points[name]>=min_value_train*0.6))*17\n",
    "    if name=='V22':\n",
    "        record22=((zhengqi_test_after_change_points[name]<=max_value_train*0.7)&(zhengqi_test_after_change_points[name]>=min_value_train*0.7))*22\n",
    "    if name=='V27':\n",
    "        record27=((zhengqi_test_after_change_points[name]<=max_value_train*0.9)&(zhengqi_test_after_change_points[name]>=min_value_train*0.7))*27\n",
    "\n",
    "record=record27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T03:13:39.214662Z",
     "start_time": "2019-04-29T03:13:39.203660Z"
    }
   },
   "outputs": [],
   "source": [
    "# result[index_of_points]=prediction1\n",
    "assert record.shape[0]==1925\n",
    "\n",
    "print(record.value_counts())\n",
    "\n",
    "# index_of_points_using1727=record==44 #use without5 trainset\n",
    "index_of_points_using27=record==27 #use without522 tarinset\n",
    "\n",
    "# index_of_points_not_use1=record!=44\n",
    "index_of_points_not_use=record!=27\n",
    "# index_of_points_not_use=index_of_points_not_use1&index_of_points_not_use2 #use without511172227 train set\n",
    "\n",
    "assert (np.sum(index_of_points_not_use)+np.sum(index_of_points_using27))==1925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T23:56:18.212136Z",
     "start_time": "2019-04-29T23:56:18.035605Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "第五步， tune训练所用的模型\n",
    "\n",
    "定义function for finding best parameters and training\n",
    "\"\"\"\n",
    "\n",
    "def find_best_parameter_for_kernel_ridge(train_x, train_y):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.kernel_ridge import KernelRidge\n",
    "    LGBM_params = {'kernel': 'poly', 'alpha':1,'gamma':None,'degree':3,'coef0':1}\n",
    "\n",
    "    back_params = {\n",
    "#         'kernel':['rbf','poly'],\n",
    "        \"alpha\": np.linspace(0.01,1,20),\n",
    "        \"gamma\": np.logspace(-2, 2, 5),\n",
    "        'degree':[2, 3, 4, 5,8,],\n",
    "        'coef0':[0.5, 1, 1.5, 2],\n",
    "    }\n",
    "    for param in back_params:\n",
    "        temp_param = {param: back_params[param]}\n",
    "        estimator = KernelRidge(**LGBM_params)\n",
    "        optimized_LGBM = GridSearchCV(estimator, param_grid=temp_param, scoring='neg_mean_squared_error',\n",
    "                                      cv=5, verbose=False, n_jobs=4)\n",
    "        optimized_LGBM.fit(train_x, train_y)\n",
    "\n",
    "        LGBM_params.update(optimized_LGBM.best_params_)\n",
    "        print('参数的最佳取值：{0}'.format(optimized_LGBM.best_params_))\n",
    "        print('最佳模型得分:{0}'.format(-optimized_LGBM.best_score_))\n",
    "    print(LGBM_params)\n",
    "\n",
    "\n",
    "def find_best_parameter_for_lgbm(train_x, train_y):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from lightgbm import LGBMRegressor\n",
    "\n",
    "    LGBM_params = {'num_leaves': 50, 'max_depth': 13, 'learning_rate': 0.1,\n",
    "                   'n_estimators': 400, 'min_child_weight': 1, 'subsample': 0.8,\n",
    "                   'colsample_bytree': 0.8, 'nthread': 4, 'objective': 'regression'}\n",
    "\n",
    "    back_params = {\n",
    "        'n_estimators': [i for i in range(400, 900, 20)],\n",
    "        'num_leaves': [i for i in range(10, 45, 5)],\n",
    "        'max_depth': [i for i in range(3, 11)],\n",
    "        'min_child_weight': [i for i in range(1, 7)],\n",
    "        'subsample': np.linspace(0.1, 0.9, 9),\n",
    "        'colsample_bytree': np.linspace(0.1, 0.9, 9),\n",
    "        'learning_rate': np.linspace(0.01, 0.2, 25),\n",
    "    }\n",
    "    for param in back_params:\n",
    "        temp_param = {param: back_params[param]}\n",
    "        estimator = LGBMRegressor(**LGBM_params)\n",
    "        optimized_LGBM = GridSearchCV(estimator, param_grid=temp_param, scoring='neg_mean_squared_error',\n",
    "                                      cv=5, verbose=False, n_jobs=4)\n",
    "        optimized_LGBM.fit(train_x, train_y)\n",
    "\n",
    "        LGBM_params.update(optimized_LGBM.best_params_)\n",
    "        print('参数的最佳取值：{0}'.format(optimized_LGBM.best_params_))\n",
    "        print('最佳模型得分:{0}'.format(-optimized_LGBM.best_score_))\n",
    "    print(LGBM_params)\n",
    "\n",
    "\n",
    "def find_best_parameter_for_xgboost(train_x, train_y):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    import xgboost as xgb\n",
    "    xgb_params = {'learning_rate': 0.1, 'n_estimators': 500,\n",
    "                  'max_depth': 5, 'min_child_weight': 1,\n",
    "                  'seed': 0, 'subsample': 0.8, 'colsample_bytree': 0.8,\n",
    "                  'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}\n",
    "\n",
    "    back_params = {\n",
    "        'n_estimators': [i for i in range(500, 800, 10)],\n",
    "        'max_depth': [i for i in range(3, 11)],\n",
    "        'min_child_weight': [i for i in range(1, 7)],\n",
    "        'gamma': np.linspace(0, 1, 11),\n",
    "        'subsample': np.linspace(0.1, 0.9, 9),\n",
    "        'colsample_bytree': np.linspace(0.1, 0.9, 9),\n",
    "        'reg_alpha': np.linspace(0.1, 3, 30),\n",
    "        'reg_lambda': np.linspace(0.1, 3, 30),\n",
    "        'learning_rate': np.linspace(0.01, 0.2, 25),\n",
    "    }\n",
    "    for param in back_params:\n",
    "        temp_param = {param: back_params[param]}\n",
    "        estimator = xgb.XGBRegressor(**xgb_params)\n",
    "        optimized_XGB = GridSearchCV(estimator, param_grid=temp_param, scoring='neg_mean_squared_error',\n",
    "                                     cv=5, verbose=False, n_jobs=4)\n",
    "        optimized_XGB.fit(train_x, train_y)\n",
    "\n",
    "        xgb_params.update(optimized_XGB.best_params_)\n",
    "        print('参数的最佳取值:{0}'.format(optimized_XGB.best_params_))\n",
    "        print('最佳模型得分:{0}'.format(-optimized_XGB.best_score_))\n",
    "    print(xgb_params)\n",
    "\n",
    "\n",
    "def find_best_parameter_for_catboost(train_x, train_y):\n",
    "    from catboost import CatBoostRegressor\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    cat_params = {'n_estimators': 82,\n",
    "                  'depth': 5,\n",
    "                  'learning_rate': 0.1,\n",
    "                  'l2_leaf_reg': 3,\n",
    "                  'loss_function': 'RMSE',\n",
    "                  'logging_level': 'Silent'}\n",
    "\n",
    "    back_params = {\n",
    "        'n_estimators': [i for i in range(400, 900, 25)],\n",
    "        'depth': [i for i in range(1, 10, 1)],\n",
    "        'learning_rate': np.linspace(0.01, 0.2, 20),\n",
    "        'l2_leaf_reg': [i for i in range(1, 6, 1)],\n",
    "    }\n",
    "    for param in back_params:\n",
    "        temp_param = {param: back_params[param]}\n",
    "        estimator = CatBoostRegressor(**cat_params)\n",
    "        optimized_CAT = GridSearchCV(estimator, param_grid=temp_param, scoring='neg_mean_squared_error',\n",
    "                                     cv=5, verbose=False, n_jobs=4)\n",
    "        optimized_CAT.fit(train_x, train_y)\n",
    "\n",
    "        cat_params.update(optimized_CAT.best_params_)\n",
    "        print('参数的最佳取值：{0}'.format(optimized_CAT.best_params_))\n",
    "        print('最佳模型得分:{0}'.format(-optimized_CAT.best_score_))\n",
    "    print(cat_params)\n",
    "    \n",
    "def find_best_para_for_gbr(train_x,train_y):\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    print(\"search for gbr******\")\n",
    "    gbr_params = {'learning_rate':0.03, 'loss':'huber', 'max_depth':3,\n",
    "              'min_impurity_decrease':0.0, 'min_samples_leaf':1, 'min_samples_split':2,\n",
    "              'n_estimators':100, 'random_state':0, 'subsample':0.8}\n",
    "    back_params = {\n",
    "        'max_depth': [i for i in range(5,15,1)],\n",
    "        'n_estimators': [i for i in range(75,500,25)],\n",
    "        'learning_rate':np.linspace(0.01,0.1,10),\n",
    "        'subsample': np.linspace(0.01,0.1,10),\n",
    "        'min_samples_leaf': [i for i in range(1,15,1)],\n",
    "        'min_samples_split': [i for i in range(2,42,2)]\n",
    "    }\n",
    "    for param in back_params:\n",
    "        temp_param = {param:back_params[param]}\n",
    "        estimator = GradientBoostingRegressor(**gbr_params)\n",
    "        optimized_gbr = GridSearchCV(estimator, param_grid = temp_param, \n",
    "                                     scoring='neg_mean_squared_error',\n",
    "                                     cv=5, verbose=False, n_jobs=4)\n",
    "        optimized_gbr.fit(train_x, train_y)\n",
    "\n",
    "        gbr_params.update(optimized_gbr.best_params_)\n",
    "        print('参数的最佳取值：{0}'.format(optimized_gbr.best_params_))\n",
    "        print('最佳模型得分:{0}'.format(-optimized_gbr.best_score_))\n",
    "    print(gbr_params)\n",
    "           \n",
    "\n",
    "def find_best_para_for_rf(train_x,train_y):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    print(\"search for random forest*****\")\n",
    "    gbr_params = { 'max_depth':3,\n",
    "               'min_samples_leaf':1, 'min_samples_split':2,\n",
    "              'n_estimators':100, 'random_state':0}\n",
    "    back_params = {\n",
    "        'max_depth': [i for i in range(5,15,1)],\n",
    "        'n_estimators': [i for i in range(75,500,25)],\n",
    "        'min_samples_leaf': [i for i in range(1,15,1)],\n",
    "        'min_samples_split': [i for i in range(2,42,2)],\n",
    "        \"max_leaf_nodes\": [i for i in range(10,100,10)],\n",
    "        \"min_weight_fraction_leaf\": np.linspace(0.05,0.3,10)\n",
    "    }\n",
    "    for param in back_params:\n",
    "        temp_param = {param:back_params[param]}\n",
    "        estimator = RandomForestRegressor(**gbr_params)\n",
    "        optimized_gbr = GridSearchCV(estimator, param_grid = temp_param, \n",
    "                                     scoring='neg_mean_squared_error',\n",
    "                                     cv=5, verbose=False, n_jobs=4)\n",
    "        optimized_gbr.fit(train_x, train_y)\n",
    "\n",
    "        gbr_params.update(optimized_gbr.best_params_)\n",
    "        print('参数的最佳取值：{0}'.format(optimized_gbr.best_params_))\n",
    "        print('最佳模型得分:{0}'.format(-optimized_gbr.best_score_))\n",
    "    print(gbr_params)\n",
    "\n",
    "def train_xgb_and_predict(parameter, train_x, train_y, test_x):\n",
    "    import xgboost as xgb\n",
    "    from xgboost import plot_importance\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    xgb_model = xgb.XGBRegressor(**parameter)\n",
    "    xgb_model.fit(train_x, train_y)\n",
    "    train_pred = xgb_model.predict(train_x)\n",
    "    test_pred = xgb_model.predict(test_x)\n",
    "    print(mean_squared_error(train_y,train_pred))\n",
    "    my_xgb_plot_importance(xgb_model,(16,8))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "def train_lgbm_and_predict(parameter, train_x, train_y, test_x):\n",
    "    from lightgbm import LGBMRegressor\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    lgbm_model = LGBMRegressor(**parameter)\n",
    "    lgbm_model.fit(train_x, train_y)\n",
    "    train_pred = lgbm_model.predict(train_x)\n",
    "    test_pred = lgbm_model.predict(test_x)\n",
    "    print(mean_squared_error(train_y,train_pred))\n",
    "    my_lgbm_plot_importance(lgbm_model,(16,8))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "def train_catboost_and_predict(parameter, train_x, train_y, test_x):\n",
    "    from catboost import CatBoostRegressor\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    cat_model = CatBoostRegressor(**parameter)\n",
    "    cat_model.fit(train_x, train_y)\n",
    "    train_pred = cat_model.predict(train_x)\n",
    "    test_pred = cat_model.predict(test_x)\n",
    "    print(mean_squared_error(train_y,train_pred))\n",
    "#     my_cat_plot_importance(cat_model,(16,8))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "def my_xgb_plot_importance(booster, figsize, **kwargs): \n",
    "    from matplotlib import pyplot as plt\n",
    "    from xgboost import plot_importance\n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax, **kwargs)\n",
    "\n",
    "\n",
    "def my_lgbm_plot_importance(booster, figsize, **kwargs): \n",
    "    from matplotlib import pyplot as plt\n",
    "    from lightgbm import plot_importance\n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax, **kwargs)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def kfold_scores(alg,x_train,y_train,is_nn=False):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import tensorflow as tf\n",
    "    kf = KFold(n_splits = 5, random_state= 1, shuffle=False)\n",
    "    predict_y = []\n",
    "    \n",
    "    callbacks_list = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_mse',\n",
    "        patience=1,\n",
    "        )\n",
    "    ]\n",
    "    if is_nn:\n",
    "        alg.save_weights('initial_weights.h5')\n",
    "        \n",
    "    for kf_train,kf_test in kf.split(x_train):\n",
    "        if is_nn:\n",
    "            alg.load_weights('initial_weights.h5')\n",
    "            alg.fit(x_train.iloc[kf_train],y_train.iloc[kf_train],epochs=10,callbacks=callbacks_list,validation_data=(x_train.iloc[kf_test],y_train.iloc[kf_test]),verbose=2)\n",
    "        else:\n",
    "            alg.fit(x_train.iloc[kf_train],y_train.iloc[kf_train])\n",
    "        y_pred_train = alg.predict(x_train.iloc[kf_test])\n",
    "        mse = mean_squared_error(y_train.iloc[kf_test],y_pred_train)\n",
    "        predict_y.append(mse)\n",
    "    \n",
    "    cv_mse=np.mean(predict_y)\n",
    "    print(\"交叉验证集MSE均值为 %s\" % (np.mean(predict_y)))  \n",
    "    return cv_mse\n",
    "\n",
    "def kfold_scores_v2(name_of_model,para,x_train,y_train):\n",
    "    kf = KFold(n_splits = 5, random_state= 1, shuffle=False)\n",
    "\n",
    "    predict_y = []\n",
    "    for kf_train,kf_test in kf.split(x_train):\n",
    "        if name_of_model=='xgb':\n",
    "            import xgboost as xgb\n",
    "            from sklearn.metrics import mean_squared_error\n",
    "            alg = xgb.XGBRegressor(**para)\n",
    "        if name_of_model=='lgbm':\n",
    "            from lightgbm import LGBMRegressor\n",
    "            from sklearn.metrics import mean_squared_error\n",
    "            alg = LGBMRegressor(**para)\n",
    "        if name_of_model=='cat':\n",
    "            from catboost import CatBoostRegressor\n",
    "            from sklearn.metrics import mean_squared_error\n",
    "            alg = CatBoostRegressor(**para)\n",
    "            \n",
    "        alg.fit(x_train.iloc[kf_train],y_train.iloc[kf_train])\n",
    "        y_pred_train = alg.predict(x_train.iloc[kf_test])\n",
    "        mse = mean_squared_error(y_train.iloc[kf_test],y_pred_train)\n",
    "        predict_y.append(mse)\n",
    "    \n",
    "    cv_mse=np.mean(predict_y)\n",
    "    print(\"交叉验证集MSE均值为 %s\" % (np.mean(predict_y)))\n",
    "    \n",
    "    return cv_mse\n",
    "\n",
    "def kfold_scores_v3(para_xgb,para_lgb,x_train,y_train,l=1,x=1):\n",
    "    kf = KFold(n_splits = 5, random_state= 1, shuffle=False)\n",
    "\n",
    "    predict_y = []\n",
    "    for kf_train,kf_test in kf.split(x_train):\n",
    "\n",
    "        import xgboost as xgb\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        alg1 = xgb.XGBRegressor(**para_xgb)\n",
    "\n",
    "        from lightgbm import LGBMRegressor\n",
    "        alg2 = LGBMRegressor(**para_lgb)\n",
    "            \n",
    "        alg1.fit(x_train.iloc[kf_train],y_train.iloc[kf_train])\n",
    "        alg2.fit(x_train.iloc[kf_train],y_train.iloc[kf_train])\n",
    "\n",
    "        y_pred_train1 = alg1.predict(x_train.iloc[kf_test])\n",
    "        y_pred_train2 = alg2.predict(x_train.iloc[kf_test])\n",
    "        y_pred_train=(y_pred_train1*x+y_pred_train2*l)/(l+x)\n",
    "        mse = mean_squared_error(y_train.iloc[kf_test],y_pred_train)\n",
    "        predict_y.append(mse)\n",
    "    \n",
    "    cv_mse=np.mean(predict_y)\n",
    "    print(\"交叉验证集MSE均值为 %s\" % (np.mean(predict_y)))\n",
    "    return cv_mse\n",
    "\n",
    "def kfold_scores_v4(para_xgb,para_lgb,alpha_ridge,x_train,y_train):\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from lightgbm import LGBMRegressor\n",
    "    from sklearn.linear_model import Ridge\n",
    "    kf = KFold(n_splits = 5, random_state= 1, shuffle=False)\n",
    "    best = [0,0,0,0,10]\n",
    "    min_mse=10\n",
    "    for x in np.linspace(0.1,1,5):\n",
    "        for l in np.linspace(0.1,1,5):\n",
    "            for la in np.linspace(0.1,1,5):\n",
    "                predict_y = []\n",
    "                for kf_train,kf_test in kf.split(x_train):\n",
    "                    alg1 = xgb.XGBRegressor(**para_xgb)\n",
    "                    alg2 = LGBMRegressor(**para_lgb)\n",
    "                    alg3=Ridge(alpha_ridge)\n",
    "\n",
    "                    alg1.fit(x_train.iloc[kf_train],y_train.iloc[kf_train])\n",
    "                    alg2.fit(x_train.iloc[kf_train],y_train.iloc[kf_train])\n",
    "                    alg3.fit(x_train.iloc[kf_train],y_train.iloc[kf_train])\n",
    "\n",
    "\n",
    "                    y_pred_train1 = alg1.predict(x_train.iloc[kf_test])\n",
    "                    y_pred_train2 = alg2.predict(x_train.iloc[kf_test])\n",
    "                    y_pred_train3 = alg3.predict(x_train.iloc[kf_test])\n",
    "\n",
    "                    y_pred_train=(y_pred_train1*x+y_pred_train2*l+y_pred_train3*la)/(la+l+x)\n",
    "                    mse = mean_squared_error(y_train.iloc[kf_test],y_pred_train)\n",
    "                    predict_y.append(mse)\n",
    "                cv_mse=np.mean(predict_y)\n",
    "                if cv_mse<min_mse:\n",
    "                    min_mse=cv_mse\n",
    "                    best=[x,l,la,min_mse]\n",
    "    print(\"交叉验证集MSE最佳均值为 %s\" % (best[3]))\n",
    "    print(\"params are:\",best)\n",
    "    return cv_mse\n",
    "\n",
    "def train_ridge(column_names,train_x=zhengqi_train_after_drop_points,test_x=zhengqi_test_after_change_points):\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    zhengqi_train_after_drop_points_try,zhengqi_test_try=drop_columns(column_names,train_x,test_x)\n",
    "    train_x_try,train_y_try=split_feature_label(zhengqi_train_after_drop_points_try,'target')\n",
    "    train_x_try_scaled,zhengqi_test_try_scaled=scale_data(train_x_try,zhengqi_test_try)\n",
    "    ridge = Ridge(random_state=len(column_names))\n",
    "    params = {\n",
    "            'alpha':np.linspace(0.01, 1, 100),\n",
    "             }\n",
    "    grid = GridSearchCV(estimator=ridge, \n",
    "                        param_grid=params, \n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=5, verbose=False)\n",
    "    kfold_scores(grid,train_x_try_scaled,train_y_try)\n",
    "    grid.fit(train_x_try_scaled,train_y_try)\n",
    "    prediction=grid.predict(zhengqi_test_try_scaled)\n",
    "    print(\"best mse during training:\"+str(grid.best_score_))\n",
    "    print(\"final whole data mse:\",mean_squared_error(grid.predict(train_x_try_scaled),train_y_try))\n",
    "    print(\"best parameter:\",grid.best_params_)\n",
    "    print(\"training set columns:\",train_x_try.columns)\n",
    "    return grid,prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T23:46:42.551253Z",
     "start_time": "2019-04-28T23:46:42.538305Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lasso feature selection\n",
    "\"\"\"\n",
    "\n",
    "def ridge_feature_selection(train_x,train_y,names=train_x_try.columns):\n",
    "    from sklearn.linear_model import Ridge\n",
    "    print(\"starting feature number is:\",len(train_x_try.columns))\n",
    "    train_x=train_x.copy()\n",
    "    train_x.columns=names\n",
    "    record_mse_for_each_round=[]\n",
    "    record_dropname_for_each_round=[]\n",
    "    global_min_mse=1\n",
    "    global_min_mse_round=-1\n",
    "    rounds=0\n",
    "    while len(train_x.columns)>20:\n",
    "        min_mse=1\n",
    "        rounds=rounds+1\n",
    "        for name in train_x.columns:\n",
    "            print(\"round{}:,drop {}*********\".format(rounds,name))\n",
    "            ridge=Ridge(random_state=len(train_x.columns))\n",
    "            train_x_drop=train_x.drop([name],axis=1)\n",
    "            cv_mse=kfold_scores(ridge,train_x_drop,train_y)\n",
    "            if cv_mse<min_mse:\n",
    "                min_mse=cv_mse\n",
    "                drop_name=name\n",
    "        record_mse_for_each_round.append(min_mse)\n",
    "        record_dropname_for_each_round.append(drop_name)\n",
    "        train_x=train_x.drop([drop_name],axis=1)\n",
    "        if min_mse<global_min_mse:\n",
    "            global_min_mse=min_mse\n",
    "            global_min_mse_round=rounds\n",
    "            columns_at_this_round=train_x.columns\n",
    "    print(\"best mse is {} at rounds {}\".format(global_min_mse,global_min_mse_round))\n",
    "    print(\"columns name at this round are:\",columns_at_this_round,\"***number is\",str(len(columns_at_this_round)))\n",
    "    plt.figure(figsize=(30,16))\n",
    "    plt.xlabel(\"Number of features dropped\")\n",
    "    plt.ylabel(\"Cross validation score\")\n",
    "    plt.plot(range(1,rounds+1), record_mse_for_each_round)\n",
    "    plt.scatter(range(1,rounds+1), record_mse_for_each_round,marker='x',color='red')\n",
    "    plt.xticks(np.arange(1, rounds+1, 1.0))\n",
    "    plt.show()\n",
    "    return record_mse_for_each_round,record_dropname_for_each_round,global_min_mse,rounds\n",
    "\n",
    "\"\"\"\n",
    "Lasso feature selection\n",
    "\"\"\"\n",
    "\n",
    "def svr_feature_selection(train_x,train_y,names=train_x_try.columns):\n",
    "    from sklearn import svm\n",
    "    print(\"starting feature number is:\",len(train_x_try.columns))\n",
    "    train_x=train_x.copy()\n",
    "    train_x.columns=names\n",
    "    record_mse_for_each_round=[]\n",
    "    record_dropname_for_each_round=[]\n",
    "    global_min_mse=1\n",
    "    global_min_mse_round=-1\n",
    "    rounds=0\n",
    "    while len(train_x.columns)>20:\n",
    "        min_mse=1\n",
    "        rounds=rounds+1\n",
    "        for name in train_x.columns:\n",
    "            print(\"round{}:,drop {}*********\".format(rounds,name))\n",
    "            ridge=svm.SVR()\n",
    "            train_x_drop=train_x.drop([name],axis=1)\n",
    "            cv_mse=kfold_scores(ridge,train_x_drop,train_y)\n",
    "            if cv_mse<min_mse:\n",
    "                min_mse=cv_mse\n",
    "                drop_name=name\n",
    "        record_mse_for_each_round.append(min_mse)\n",
    "        record_dropname_for_each_round.append(drop_name)\n",
    "        train_x=train_x.drop([drop_name],axis=1)\n",
    "        if min_mse<global_min_mse:\n",
    "            global_min_mse=min_mse\n",
    "            global_min_mse_round=rounds\n",
    "            columns_at_this_round=train_x.columns\n",
    "    print(\"best mse is {} at rounds {}\".format(global_min_mse,global_min_mse_round))\n",
    "    print(\"columns name at this round are:\",columns_at_this_round,\"***number is\",str(len(columns_at_this_round)))\n",
    "    plt.figure(figsize=(30,16))\n",
    "    plt.xlabel(\"Number of features dropped\")\n",
    "    plt.ylabel(\"Cross validation score\")\n",
    "    plt.plot(range(1,rounds+1), record_mse_for_each_round)\n",
    "    plt.scatter(range(1,rounds+1), record_mse_for_each_round,marker='x',color='red')\n",
    "    plt.xticks(np.arange(1, rounds+1, 1.0))\n",
    "    plt.show()\n",
    "    return record_mse_for_each_round,record_dropname_for_each_round,global_min_mse,rounds\n",
    "\n",
    "def bridge_feature_selection(train_x,train_y,names=train_x_try.columns):\n",
    "    from sklearn.linear_model import BayesianRidge\n",
    "    print(\"starting feature number is:\",len(train_x_try.columns))\n",
    "    train_x=train_x.copy()\n",
    "    train_x.columns=names\n",
    "    record_mse_for_each_round=[]\n",
    "    record_dropname_for_each_round=[]\n",
    "    global_min_mse=1\n",
    "    global_min_mse_round=-1\n",
    "    rounds=0\n",
    "    while len(train_x.columns)>20:\n",
    "        min_mse=1\n",
    "        rounds=rounds+1\n",
    "        for name in train_x.columns:\n",
    "            print(\"round{}:,drop {}*********\".format(rounds,name))\n",
    "            ridge=BayesianRidge()\n",
    "            train_x_drop=train_x.drop([name],axis=1)\n",
    "            cv_mse=kfold_scores(ridge,train_x_drop,train_y)\n",
    "            if cv_mse<min_mse:\n",
    "                min_mse=cv_mse\n",
    "                drop_name=name\n",
    "        record_mse_for_each_round.append(min_mse)\n",
    "        record_dropname_for_each_round.append(drop_name)\n",
    "        train_x=train_x.drop([drop_name],axis=1)\n",
    "        if min_mse<global_min_mse:\n",
    "            global_min_mse=min_mse\n",
    "            global_min_mse_round=rounds\n",
    "            columns_at_this_round=train_x.columns\n",
    "    print(\"best mse is {} at rounds {}\".format(global_min_mse,global_min_mse_round))\n",
    "    print(\"columns name at this round are:\",columns_at_this_round,\"***number is\",str(len(columns_at_this_round)))\n",
    "    plt.figure(figsize=(30,16))\n",
    "    plt.xlabel(\"Number of features dropped\")\n",
    "    plt.ylabel(\"Cross validation score\")\n",
    "    plt.plot(range(1,rounds+1), record_mse_for_each_round)\n",
    "    plt.scatter(range(1,rounds+1), record_mse_for_each_round,marker='x',color='red')\n",
    "    plt.xticks(np.arange(1, rounds+1, 1.0))\n",
    "    plt.show()\n",
    "    return record_mse_for_each_round,record_dropname_for_each_round,global_min_mse,rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "第一次尝试的方案，通过训练集的最大最小值把测试集分成两个部分，符合训练集的部分以及超过训练集的部分，\n",
    "然后对这两个测试集分别进行预测，最后通过numpy mask整合在一起效果不好\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T03:25:43.459025Z",
     "start_time": "2019-04-29T03:25:43.455036Z"
    }
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(final_prediction_xgb_lgbm_ridge_scaled_feature_transformed_5_and_527)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T23:20:07.335584Z",
     "start_time": "2019-04-25T23:16:18.134902Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1\n",
    "归一化的版本，lgmb，使用了11172227这些column\n",
    "\"\"\"\n",
    "\n",
    "find_best_parameter_for_lgbm(train_x_without5_scaled,train_y_without5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T20:01:11.247725Z",
     "start_time": "2019-04-26T20:01:07.159693Z"
    }
   },
   "outputs": [],
   "source": [
    "para={'num_leaves': 20, 'max_depth': 3, 'learning_rate': 0.04, 'n_estimators': 400, 'min_child_weight': 1, 'subsample': 0.1, 'colsample_bytree': 0.8, 'nthread': 4, 'objective': 'regression'}\n",
    "train_pred21,test_pred21=train_lgbm_and_predict(para,train_x_without5_scaled,train_y_without5,zhengqi_test_without5_scaled)\n",
    "kfold_scores_v2('lgbm',para,train_x_without5_scaled,train_y_without5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T01:44:23.611496Z",
     "start_time": "2019-04-26T01:44:23.604440Z"
    }
   },
   "outputs": [],
   "source": [
    "# store the final result for lgbm with scaled\n",
    "final_prediction_lgbm_scaled=np.zeros((zhengqi_test.shape[0],1))\n",
    "\n",
    "final_prediction_lgbm_scaled[index_of_points_using11172227]=test_pred21[index_of_points_using11172227].reshape(-1,1)\n",
    "final_prediction_lgbm_scaled[index_of_points_using111727]=test_pred22[index_of_points_using111727].reshape(-1,1)\n",
    "final_prediction_lgbm_scaled[index_of_points_not_use]=test_pred23[index_of_points_not_use].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T00:33:45.385578Z",
     "start_time": "2019-04-26T00:08:04.203030Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1\n",
    "归一化的版本，xgboost，使用了11172227这些column\n",
    "\"\"\"\n",
    "\n",
    "find_best_parameter_for_xgboost(train_x_without5_scaled,train_y_without5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T20:08:34.907844Z",
     "start_time": "2019-04-26T20:08:01.417426Z"
    }
   },
   "outputs": [],
   "source": [
    "para={'learning_rate': 0.06, 'n_estimators': 510, 'max_depth': 5, 'min_child_weight': 2, 'seed': 0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.0, 'reg_alpha': 0.6, 'reg_lambda': 1.7}\n",
    "train_pred41,test_pred41=train_xgb_and_predict(para,train_x_without5_scaled,train_y_without5,zhengqi_test_without5_scaled)\n",
    "kfold_scores_v2('xgb',para,train_x_without5_scaled,train_y_without5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T03:59:09.079201Z",
     "start_time": "2019-04-26T03:59:09.072291Z"
    }
   },
   "outputs": [],
   "source": [
    "# store the final result for xgboost with scaled\n",
    "final_prediction_xgb_scaled=np.zeros((zhengqi_test.shape[0],1))\n",
    "\n",
    "final_prediction_xgb_scaled[index_of_points_using11172227]=test_pred41[index_of_points_using11172227].reshape(-1,1)\n",
    "final_prediction_xgb_scaled[index_of_points_using111727]=test_pred42[index_of_points_using111727].reshape(-1,1)\n",
    "final_prediction_xgb_scaled[index_of_points_not_use]=test_pred43[index_of_points_not_use].reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T03:59:33.981117Z",
     "start_time": "2019-04-26T03:59:33.955376Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "最后一步，整合所有结果，并存储\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "scaled result\n",
    "\"\"\"\n",
    "result_xgb_lgbm_scaled_average=(final_prediction_xgb_scaled+final_prediction_lgbm_scaled)/2.0\n",
    "np.savetxt('result_xgb_lgbm_scaled_average.txt',result_xgb_lgbm_scaled_average)\n",
    "\n",
    "print(result_xgb_lgbm_scaled_average.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T22:29:11.834245Z",
     "start_time": "2019-04-27T22:26:36.895854Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "第二种尝试方案，训练多个模型，然后用库自带的stacking方法来集成或神经网络集成，效果也不好\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso,LinearRegression,Ridge\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "para_xgb={'learning_rate': 0.07333333333333333, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 1, 'seed': 0, 'subsample': 0.8, 'colsample_bytree': 0.4, 'gamma': 0.9, 'reg_alpha': 0.1, 'reg_lambda': 0.1}\n",
    "para_lgb={'num_leaves': 10, 'max_depth': 3, 'learning_rate': 0.025833333333333333, 'n_estimators': 400, 'min_child_weight': 1, 'subsample': 0.1, 'colsample_bytree': 0.4, 'nthread': 4, 'objective': 'regression'}\n",
    "alpha_ridge=0.06\n",
    "\n",
    "alg1 = xgb.XGBRegressor(**para_xgb)\n",
    "alg2 = LGBMRegressor(**para_lgb)\n",
    "alg3=Ridge(alpha_ridge)\n",
    "\n",
    "\n",
    "####1SVM回归####\n",
    "from sklearn import svm\n",
    "model_SVR = svm.SVR()\n",
    "####4GBRT回归####\n",
    "from sklearn import ensemble\n",
    "para_gbr={'learning_rate': 0.020000000000000004, 'loss': 'huber', 'max_depth': 5, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 2, 'min_samples_split': 20, 'n_estimators': 300, 'random_state': 0, 'subsample': 0.09000000000000001}\n",
    "model_GradientBoostingRegressor = ensemble.GradientBoostingRegressor(**para_gbr)\n",
    "####5BayesianRidge贝叶斯岭回归\n",
    "from sklearn.linear_model import BayesianRidge,TheilSenRegressor\n",
    "model_BayesianRidge = BayesianRidge()\n",
    "####6TheilSen泰尔森估算\n",
    "model_TheilSenRegressor = TheilSenRegressor(n_jobs=2)\n",
    "# 7\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "model_KernelRidge=KernelRidge(alpha=0.06, kernel='polynomial', degree=3, coef0=1)\n",
    "regressors = [alg1,alg2,alg3,model_KernelRidge,model_SVR,model_TheilSenRegressor,model_BayesianRidge,model_GradientBoostingRegressor]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "gbr_params = {'learning_rate':0.03, 'loss':'huber', 'max_depth':3,\n",
    "          'min_impurity_decrease':0.0, 'min_samples_leaf':1, 'min_samples_split':2,\n",
    "          'n_estimators':100, 'random_state':0, 'subsample':0.8}\n",
    "back_params = {\n",
    "    'meta-gradientboostingregressor__max_depth': [i for i in range(5,15,1)],\n",
    "    'meta-gradientboostingregressor__n_estimators': [i for i in range(75,500,25)],\n",
    "    'meta-gradientboostingregressor__learning_rate':np.linspace(0.01,0.1,10),\n",
    "    'meta-gradientboostingregressor__subsample': np.linspace(0.01,0.1,10),\n",
    "    'meta-gradientboostingregressor__min_samples_leaf': [i for i in range(1,15,2)],\n",
    "    'meta-gradientboostingregressor__min_samples_split': [i for i in range(2,38,4)]\n",
    "}\n",
    "\n",
    "meta_gbr = GradientBoostingRegressor(**gbr_params)\n",
    "\n",
    "regressors = [alg1,alg2,alg3,model_KernelRidge,model_SVR,model_TheilSenRegressor,model_BayesianRidge,model_GradientBoostingRegressor]\n",
    "\n",
    "stregr = StackingRegressor(regressors=regressors, \n",
    "                           meta_regressor=meta_gbr)\n",
    "\n",
    "grid = GridSearchCV(estimator=stregr, \n",
    "                    param_grid=back_params, \n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    cv=5, verbose=False, n_jobs=4)\n",
    "grid.fit(train_x_try_scaled, train_y_try)\n",
    "\n",
    "print(\"best score\",grid.best_score_)\n",
    "print(\"best estimator\",grid.best_estimator_)\n",
    "print(\"best params\",grid.best_params_)\n",
    "\n",
    "# lr = LinearRegression()\n",
    "# svr_lin = SVR(kernel='linear')\n",
    "# ridge = Ridge(random_state=1)\n",
    "# svr_rbf = SVR(kernel='rbf')\n",
    "\n",
    "# stregr = StackingRegressor(regressors=[svr_lin, lr, ridge], \n",
    "#                            meta_regressor=svr_rbf)\n",
    "\n",
    "# stregr.fit(X, y)\n",
    "# stregr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T04:35:03.985171Z",
     "start_time": "2019-04-23T04:35:03.873610Z"
    }
   },
   "outputs": [],
   "source": [
    "result=model_nn.predict(zhengqi_test_after_drop_columns_scaled)\n",
    "np.savetxt('result.txt',result)\n",
    "\n",
    "k = result.tolist()\n",
    "\n",
    "with open('data.txt','w') as f:\n",
    "    for i in k:\n",
    "        f.write(str(i) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "def build_model(input_shape,num_of_layers,num_of_units,dropout_rate):\n",
    "    # Because we will need to instantiate\n",
    "    # the same model multiple times,\n",
    "    # we use a function to construct it.\n",
    "    assert num_of_layers>2\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(num_of_units, activation='relu',\n",
    "                               input_shape=(input_shape,)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.BatchNormalization())\n",
    "#     add middle layers\n",
    "    for i in range(num_of_layers-2):\n",
    "        model.add(layers.Dense(num_of_units, activation='relu'))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        \n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# model.fit(train_x_scaled, train_y,\n",
    "#               epochs=5, batch_size=64, verbose=0)\n",
    "# print(model.metrics_names)\n",
    "# model.evaluate(test_x_scaled,test_y)\n",
    "\n",
    "min_cv_mse=10\n",
    "min_test_mse=10\n",
    "min_cv_mse_parameters=[]\n",
    "min_test_mse_parameters=[]\n",
    "for num_of_layers in [3,4,5]:\n",
    "    for num_of_units in [128,256,512]:\n",
    "        for drop_out_rate in [0,0.1]:\n",
    "            print('num of layers is {}, num of units is {}, dropout rate is {}'.format(num_of_layers,num_of_units,drop_out_rate))\n",
    "            cv_mse_list=[]\n",
    "            test_mse_list=[]\n",
    "            model=build_model(results_of_models.shape[1],num_of_layers,num_of_units,drop_out_rate)\n",
    "            cv_mse=kfold_scores(model,results_of_models,train_y_try,True)\n",
    "            cv_mse_list.append(cv_mse)\n",
    "            if np.mean(cv_mse_list)<min_cv_mse:\n",
    "                min_cv_mse=np.mean(cv_mse_list)\n",
    "                min_cv_mse_parameters=[min_cv_mse,num_of_layers,num_of_units,drop_out_rate]\n",
    "#             if np.mean(test_mse)<min_test_mse:\n",
    "#                 min_test_mse=np.mean(test_mse_list)\n",
    "#                 min_test_mse_parameters=[min_test_mse,num_of_layers,num_of_units,drop_out_rate]\n",
    "\n",
    "print(min_cv_mse)\n",
    "print(min_cv_mse_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T00:27:44.768711Z",
     "start_time": "2019-04-25T00:27:44.717872Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "可选，没用到，遍历所有可能的组合并生成相对应的训练和测试集\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def drop_columns(list_of_columns,train_data_frame,test_data_frame):\n",
    "    zhengqi_train_after_drop=train_data_frame.drop(list_of_columns,axis=1)\n",
    "    zhengqi_test_after_drop=test_data_frame.drop(list_of_columns,axis=1)\n",
    "    return zhengqi_train_after_drop,zhengqi_test_after_drop\n",
    "\n",
    "# remove V5\n",
    "zhengqi_train_after_drop_points,_=drop_columns(['V5'],zhengqi_train_after_drop_points,zhengqi_test)\n",
    "\n",
    "from itertools import combinations\n",
    "columns=['V11','V17','V22','V27']\n",
    "all_combinations=[]\n",
    "all_combination_data={}\n",
    "for i in range(len(columns)):\n",
    "    all_combinations.append(list(combinations(columns,i+1)))\n",
    "\n",
    "for i in range(len(all_combinations)):\n",
    "    for j in range(len(all_combinations[i])):\n",
    "        key_train='train_without_'\n",
    "        key_test='test_without_'\n",
    "        record=''\n",
    "        for name in all_combinations[i][j]:\n",
    "            record=record+name\n",
    "        key_train=key_train+record\n",
    "        key_test=key_test+record\n",
    "        all_combination_data[key_train],all_combination_data[key_test]=drop_columns(list(all_combinations[i][j]),zhengqi_train_after_drop_points,zhengqi_test)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "第三种方案，先去掉一些分布严重不一致的特征，然后分箱（分或者不分），然后组合，\n",
    "然后用ridge或者bayesian ridge通过backward feature selection来筛选特征\n",
    "这种方案根据是否分箱，以及如何组合特征，有多个可能性\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_mse_for_each_round,record_dropname_for_each_round,global_min_mse,rounds=ridge_feature_selection(train_x_try_scaled,train_y_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_mse_for_each_round,record_dropname_for_each_round,global_min_mse,rounds=bridge_feature_selection(train_x_try_scaled,train_y_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "根据上面两个的结果，选择特征\n",
    "\"\"\"\n",
    "dropped_columns_2=['V5','V17','V22','V27','V35V14', 'SquaredV15', 'V28V19', 'V23V14', 'V9V15', 'V24V1', 'V24V16', 'V24V29', 'V11', 'V23V16', 'V18', 'SquaredV3', 'V20', 'V24V10', 'V35V10', 'V23V29', 'SquaredV19', 'V35V15', 'V21', 'V23', 'V9V19', 'SquaredV0', 'V28V3', 'V24V36', 'V28V16', 'V24V0', 'V35V30', 'V9V30', 'V28V0', 'V9V29', 'V35V3', 'V28V30', 'V24V3', 'V35V36', 'V19', 'V9V10', 'SquaredV29', 'V9V3', 'V9V37', 'V35V19', 'V28V36', 'V25', 'V23V37', 'V24V14', 'SquaredV14', 'V24V37', 'V28', 'V28V37', 'V24V30', 'V30', 'V9V1', 'V23V19', 'V23V1', 'V24', 'SquaredV30', 'V35V29', 'V9V14', 'V29', 'V24V19', 'V34', 'V23V15', 'V28V10', 'V28V14', 'V26', 'V32', 'V6', 'V23V30', 'V35', 'V15', 'V9V0', 'V28V1', 'V31', 'V35V1', 'V35V37']\n",
    "zhengqi_train_after_drop_points_try_2,zhengqi_test_try_2=drop_columns(dropped_columns_2,zhengqi_train_after_drop_points,zhengqi_test_after_change_points)\n",
    "train_x_try_2,train_y_try_2=split_feature_label(zhengqi_train_after_drop_points_try_2,'target')\n",
    "train_x_try_scaled_2,zhengqi_test_try_scaled_2=scale_data(train_x_try_2,zhengqi_test_try_2)\n",
    "print(zhengqi_test_try_2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "看看用了哪些特征，哪些没用\n",
    "\"\"\"\n",
    "name_set={\"V\"}\n",
    "for name in zhengqi_test_try_2.columns:\n",
    "    for subname in zhengqi_test.columns:\n",
    "        if subname in name:\n",
    "            name_set.add(subname)\n",
    "            \n",
    "name_l=list(name_set)\n",
    "print(name_l)\n",
    "print(set(zhengqi_test.columns)-name_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看看筛选后的特征，训练集和测试集是否分布一致\n",
    "plot_each_column(zhengqi_train_after_drop_points_try_2,zhengqi_test_try_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune模型参数\n",
    "dropped_columns_2=['V5','V17','V22','V27','V35V14', 'SquaredV15', 'V28V19', 'V23V14', 'V9V15', 'V24V1', 'V24V16', 'V24V29', 'V11', 'V23V16', 'V18', 'SquaredV3', 'V20', 'V24V10', 'V35V10', 'V23V29', 'SquaredV19', 'V35V15', 'V21', 'V23', 'V9V19', 'SquaredV0', 'V28V3', 'V24V36', 'V28V16', 'V24V0', 'V35V30', 'V9V30', 'V28V0', 'V9V29', 'V35V3', 'V28V30', 'V24V3', 'V35V36', 'V19', 'V9V10', 'SquaredV29', 'V9V3', 'V9V37', 'V35V19', 'V28V36', 'V25', 'V23V37', 'V24V14', 'SquaredV14', 'V24V37', 'V28', 'V28V37', 'V24V30', 'V30', 'V9V1', 'V23V19', 'V23V1', 'V24', 'SquaredV30', 'V35V29', 'V9V14', 'V29', 'V24V19', 'V34', 'V23V15', 'V28V10', 'V28V14', 'V26', 'V32', 'V6', 'V23V30', 'V35', 'V15', 'V9V0', 'V28V1', 'V31', 'V35V1', 'V35V37']\n",
    "train_ridge(dropped_columns_2,zhengqi_train_after_drop_points,zhengqi_test_after_change_points)\n",
    "find_best_parameter_for_kernel_ridge(train_x_try_scaled_2,train_y_try_2)\n",
    "print(train_x_try_2.columns)\n",
    "find_best_parameter_for_lgbm(train_x_try_scaled_2,train_y_try)\n",
    "find_best_parameter_for_xgboost(train_x_try_scaled_2,train_y_try)\n",
    "find_best_para_for_gbr(train_x_try_scaled_2,train_y_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "只用两个，效果不好\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "bayesian ridge and ridge\n",
    "\"\"\"\n",
    "def kfold_scores_v6(x_train,y_train):\n",
    "    kf = KFold(n_splits = 5, random_state= 1, shuffle=False)\n",
    "    best = [0,0,10]\n",
    "    min_mse=10\n",
    "    i=0\n",
    "    for x1 in np.linspace(0.1,1,10):\n",
    "        for x2 in np.linspace(0.1,1,10):\n",
    "#             for x3 in np.linspace(0.1,1,10):\n",
    "#             for x4 in np.linspace(0.1,1,5):\n",
    "            predict_y = []\n",
    "            for kf_train,kf_test in kf.split(x_train):\n",
    "                alg1 = BayesianRidge()\n",
    "                alg2=Ridge(0.51)\n",
    "#                             alg3 = BayesianRidge()\n",
    "#                         alg3=KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "#                     alg3 = TheilSenRegressor()\n",
    "#                     para_xgb={'learning_rate': 0.025833333333333333, 'n_estimators': 500, 'max_depth': 3, 'min_child_weight': 2, 'seed': 0, 'subsample': 0.6, 'colsample_bytree': 0.2, 'gamma': 0.9, 'reg_alpha': 0.2, 'reg_lambda': 1.2}\n",
    "#                     alg3 = xgb.XGBRegressor(**para_xgb)\n",
    "\n",
    "                alg1.fit(x_train.iloc[kf_train],y_train.iloc[kf_train])\n",
    "                alg2.fit(x_train.iloc[kf_train],y_train.iloc[kf_train])\n",
    "#                     alg3.fit(x_train.iloc[kf_train],y_train.iloc[kf_train])\n",
    "#                         alg4.fit(x_train.iloc[kf_train],y_train.iloc[kf_train])\n",
    "#                             alg5.fit(x_train.iloc[kf_train],y_train.iloc[kf_train])\n",
    "\n",
    "\n",
    "                y_pred_train1 = alg1.predict(x_train.iloc[kf_test])\n",
    "                y_pred_train2 = alg2.predict(x_train.iloc[kf_test])\n",
    "#                     y_pred_train3 = alg3.predict(x_train.iloc[kf_test])\n",
    "#                         y_pred_train4 = alg4.predict(x_train.iloc[kf_test])\n",
    "#                             y_pred_train5 = alg5.predict(x_train.iloc[kf_test])\n",
    "\n",
    "\n",
    "                y_pred_train=(y_pred_train1*x1+y_pred_train2*x2)/(x1+x2)\n",
    "                mse = mean_squared_error(y_train.iloc[kf_test],y_pred_train)\n",
    "                predict_y.append(mse)\n",
    "            cv_mse=np.mean(predict_y)\n",
    "            print(\"current turn mse:\",cv_mse,\"turn is\",i)\n",
    "            i=i+1\n",
    "            if cv_mse<min_mse:\n",
    "                min_mse=cv_mse\n",
    "                best=[x1,x2,min_mse]\n",
    "#     print(\"交叉验证集MSE最佳均值为 %s\" % (best[3]))\n",
    "    print(\"params are:\",best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bayesian ridge and ridge\n",
    "\"\"\"\n",
    "print(len(train_x_try_2.columns))\n",
    "kfold_scores_v6(train_x_try_scaled_2,train_y_try)#bayesian ridge and ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "用八个，最佳\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso,LinearRegression,Ridge\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "para_xgb={'learning_rate': 0.07333333333333333, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 1, 'seed': 0, 'subsample': 0.8, 'colsample_bytree': 0.4, 'gamma': 0.9, 'reg_alpha': 0.1, 'reg_lambda': 0.1}\n",
    "para_lgb={'num_leaves': 10, 'max_depth': 3, 'learning_rate': 0.025833333333333333, 'n_estimators': 400, 'min_child_weight': 1, 'subsample': 0.1, 'colsample_bytree': 0.4, 'nthread': 4, 'objective': 'regression'}\n",
    "alpha_ridge=0.06\n",
    "\n",
    "alg1 = xgb.XGBRegressor(**para_xgb)\n",
    "alg2 = LGBMRegressor(**para_lgb)\n",
    "alg3=Ridge(alpha_ridge)\n",
    "\n",
    "\n",
    "####1SVM回归####\n",
    "from sklearn import svm\n",
    "model_SVR = svm.SVR()\n",
    "####4GBRT回归####\n",
    "from sklearn import ensemble\n",
    "para_gbr={'learning_rate': 0.020000000000000004, 'loss': 'huber', 'max_depth': 5, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 2, 'min_samples_split': 20, 'n_estimators': 300, 'random_state': 0, 'subsample': 0.09000000000000001}\n",
    "model_GradientBoostingRegressor = ensemble.GradientBoostingRegressor(**para_gbr)\n",
    "####5BayesianRidge贝叶斯岭回归\n",
    "from sklearn.linear_model import BayesianRidge,TheilSenRegressor\n",
    "model_BayesianRidge = BayesianRidge()\n",
    "####6TheilSen泰尔森估算\n",
    "model_TheilSenRegressor = TheilSenRegressor(n_jobs=2)\n",
    "# 7\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "model_KernelRidge=KernelRidge(alpha=0.06, kernel='polynomial', degree=3, coef0=1)\n",
    "regressors = [alg1,alg2,alg3,model_KernelRidge,model_SVR,model_TheilSenRegressor,model_BayesianRidge,model_GradientBoostingRegressor]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看看每个单独模型的预测结果\n",
    "results_of_models=pd.DataFrame()\n",
    "# regressors = [alg1,alg2,alg3,model_KernelRidge,model_SVR,model_TheilSenRegressor,model_BayesianRidge,]\n",
    "i=0\n",
    "for model in regressors:\n",
    "    print(model)\n",
    "    kfold_scores(model,train_x_try_scaled_2,train_y_try)\n",
    "    model.fit(train_x_try_scaled_2,train_y_try)\n",
    "    mean_squared_error(model.predict(train_x_try_scaled_2),train_y_try)\n",
    "    result=model.predict(zhengqi_test_try_scaled)\n",
    "    results_of_models[str(i)]=result\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T00:26:47.382057Z",
     "start_time": "2019-05-02T00:26:47.357194Z"
    }
   },
   "outputs": [],
   "source": [
    "def kfold_scores_v8(x_train,y_train,regressors):\n",
    "    kf = KFold(n_splits = 5, random_state= 1, shuffle=True)\n",
    "    best = []\n",
    "    kfold_results=[]\n",
    "    record_of_index_train=[]\n",
    "    record_of_index_test=[]\n",
    "    min_mse=10\n",
    "    turn=0\n",
    "    for x0 in np.linspace(0.1,1,5):\n",
    "        for x1 in np.linspace(0.1,1,5):\n",
    "            for x2 in np.linspace(0.1,1,5):\n",
    "                for x3 in np.linspace(0.1,1,5):\n",
    "                    for x4 in np.linspace(0.1,1,5):\n",
    "                        for x5 in np.linspace(0.1,1,5):\n",
    "                            for x6 in np.linspace(0.1,1,5):\n",
    "                                for x7 in np.linspace(0.1,1,5):\n",
    "                                    predict_y = []\n",
    "                                    if turn==0:               \n",
    "                                        for kf_train,kf_test in kf.split(x_train):\n",
    "                                            record_of_index_train.append(kf_train)\n",
    "                                            record_of_index_test.append(kf_test)\n",
    "                                            predictions=pd.DataFrame()\n",
    "                                            j=0\n",
    "                                            for model in regressors:\n",
    "                                                model.fit(x_train.iloc[kf_train],y_train.iloc[kf_train])\n",
    "                                                one_model_prediction=model.predict(x_train.iloc[kf_test])\n",
    "                                                predictions[str(j)]=one_model_prediction\n",
    "                                                j=j+1\n",
    "                                            kfold_results.append(predictions)\n",
    "                                        for i,prediction in enumerate(kfold_results):\n",
    "                                            y_pred_train=(prediction.iloc[:,0]*x0+prediction.iloc[:,1]*x1+prediction.iloc[:,2]*x2+prediction.iloc[:,3]*x3+prediction.iloc[:,4]*x4+prediction.iloc[:,5]*x5\n",
    "                                            +prediction.iloc[:,6]*x6+prediction.iloc[:,7]*x7)/(x0+x1+x2+x3+x4+x5+x6+x7)\n",
    "                                            kf_test=record_of_index_test[i]\n",
    "                                            mse = mean_squared_error(y_train.iloc[kf_test],y_pred_train)\n",
    "                                            predict_y.append(mse)\n",
    "                                    else:#以后只直接调用第一次算好的结果)\n",
    "                                        for i,prediction in enumerate(kfold_results):\n",
    "                                            y_pred_train=(prediction.iloc[:,0]*x0+prediction.iloc[:,1]*x1+prediction.iloc[:,2]*x2+prediction.iloc[:,3]*x3+prediction.iloc[:,4]*x4+prediction.iloc[:,5]*x5\n",
    "                                            +prediction.iloc[:,6]*x6+prediction.iloc[:,7]*x7)/(x0+x1+x2+x3+x4+x5+x6+x7)\n",
    "                                            kf_test=record_of_index_test[i]\n",
    "                                            mse = mean_squared_error(y_train.iloc[kf_test],y_pred_train)\n",
    "                                            predict_y.append(mse)\n",
    "                                    assert len(predict_y)==5\n",
    "                                    cv_mse=np.mean(predict_y)\n",
    "                                    print(\"current turn mse:\",cv_mse,\"turn is\",turn)\n",
    "                                    print(best)\n",
    "                                    turn=turn+1\n",
    "                                    if cv_mse<min_mse:\n",
    "                                        min_mse=cv_mse\n",
    "                                        best=[x0,x1,x2,x3,x4,x5,x6,x7,min_mse]\n",
    "#     print(\"交叉验证集MSE最佳均值为 %s\" % (best[3]))\n",
    "    print(\"params are:\",best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_scores_v8(train_x_try_scaled_2,train_y_try_2,regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zhengqi_test_try_2.columns)\n",
    "print(train_x_try_2.columns)\n",
    "alg1.fit(train_x_try_scaled_2,train_y_try_2)\n",
    "alg2.fit(train_x_try_scaled_2,train_y_try_2)\n",
    "alg3.fit(train_x_try_scaled_2,train_y_try_2)\n",
    "model_KernelRidge.fit(train_x_try_scaled_2,train_y_try_2)\n",
    "model_SVR.fit(train_x_try_scaled_2,train_y_try_2)\n",
    "model_TheilSenRegressor.fit(train_x_try_scaled_2,train_y_try_2)\n",
    "model_BayesianRidge.fit(train_x_try_scaled_2,train_y_try_2)\n",
    "model_GradientBoostingRegressor.fit(train_x_try_scaled_2,train_y_try_2)\n",
    "r1=alg1.predict(zhengqi_test_try_scaled_2)\n",
    "r2=alg2.predict(zhengqi_test_try_scaled_2)\n",
    "r3=alg3.predict(zhengqi_test_try_scaled_2)\n",
    "r4=model_KernelRidge.predict(zhengqi_test_try_scaled_2)\n",
    "r5=model_SVR.predict(zhengqi_test_try_scaled_2)\n",
    "r6=model_TheilSenRegressor.predict(zhengqi_test_try_scaled_2)\n",
    "r7=model_BayesianRidge.predict(zhengqi_test_try_scaled_2)\n",
    "r8=model_GradientBoostingRegressor.predict(zhengqi_test_try_scaled_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T00:27:18.655112Z",
     "start_time": "2019-05-02T00:27:18.651156Z"
    }
   },
   "outputs": [],
   "source": [
    "# 根据上面的权重，计算最终结果\n",
    "ultimate_result=(0.1*r1+0.55*r2+1*r3+ 0.1*r4+ 0.1*r5 +0.55*r6 +1*r7 +0.55*r8)/(0.1+0.55+ 1.0+ 0.1+ 0.1+ 0.55+ 1.0+ 0.55)\n",
    "np.savetxt(\"eight_model_stacked_without511172227_with_feature_combined_no_binned_scaled.txt\",ultimate_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "看看不同方案的预测结果之间分布\n",
    "\"\"\"\n",
    "no_transformed=np.loadtxt(\"eight_model_stacked_without511172227_no_feature_combined_no_binned_scaled.txt\")\n",
    "transformed=np.loadtxt(\"eight_model_stacked_without511172227_with_feature_combined_with_binned_scaled.txt\")\n",
    "plt.figure(figsize=(400,26))\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"values\")\n",
    "# plt.plot(range(1,len(y_pred_train_svr_ridge)+1), y_pred_train_svr_ridge)\n",
    "plt.scatter(range(1,len(y_pred_train_svr_ridge)+1), y_pred_train_svr_ridge,marker='x',color='red')\n",
    "# plt.plot(range(1,len(y_pred_train_svr_ridge)+1), y_pred_train_svr_ridge_xgb)\n",
    "plt.scatter(range(1,len(y_pred_train_svr_ridge)+1), y_pred_train_svr_ridge_xgb,marker='o',color='blue')\n",
    "plt.scatter(range(1,len(y_pred_train_svr_ridge)+1), y_pred_train2,marker='v',color='yellow')\n",
    "plt.scatter(range(1,len(y_pred_train_svr_ridge)+1), no_transformed,marker='o',color='green')\n",
    "plt.scatter(range(1,len(y_pred_train_svr_ridge)+1), transformed,marker='v',color='brown')\n",
    "plt.legend(['svr_ridge','svr_ridge_xgb','ridge','e1','e2'])\n",
    "\n",
    "\n",
    "# plt.xticks(np.arange(1, rounds+1, 1.0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shape_predict_v2",
   "language": "python",
   "name": "shape_predict_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
